{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454e37d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/figs/bayes_theorem.png\" width=\"400\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "# <center>Naive Bayes Classifiers <a class=\"tocSkip\"></center>\n",
    "## <center>Davi Moreira <a class=\"tocSkip\"></center>\n",
    "### <center>Quantitative Theory and Methods <a class=\"tocSkip\"></center>\n",
    "### <center>Emory University <a class=\"tocSkip\"></center>\n",
    "    \n",
    "<!---Notebook Navigation: [Another Cell](#another_cell)\n",
    "Slide Navigation: <a href=\"#/2/3\">Link to xxx</a>--->\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b88448b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/figs/xxx.png\" width=\"400\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "# <center> Access the link to follow the material <a class=\"tocSkip\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3ca39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's Summary and Learning Objectives\n",
    "\n",
    "1. **Statistical Foundations** of Naive Bayes Classifiers.\n",
    "\n",
    "        - The beauty of Bayes Theorem in action;\n",
    "        - The Naive Bayes assumptions;\n",
    "        - Three versions of Naive Bayes Classifiers:\n",
    "                - Gaussian Naive Bayes.\n",
    "                - Multinomial Naive Bayes.\n",
    "                - Bernoulli Naive Bayes.\n",
    "\n",
    "> \"Knowing the underlying statistics allows us to **implement and interpret model outputs** more effectively.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f35576",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's Summary and Learning Objectives <a class=\"tocSkip\">\n",
    "\n",
    "\n",
    "2. Naive Bayes Classifiers **Implementation with `Python`**.\n",
    "\n",
    "        - Real-world classification problems.\n",
    "        - Why Python? A versatile tool for data science and machine learning.\n",
    "        - Python's Libraries Make Life Easier: NumPy, pandas, scikit-learn, and others.\n",
    "\n",
    "> \"Understanding the code and computational aspects enables us to **apply Naive Bayes to real-world problems** efficiently.\"\n",
    "\n",
    "### Let's embark on a journey through the realms of probability, statistics, and machine learning! <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5724caf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>What is a classification problem?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf289d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>What is a classification problem? <a class=\"tocSkip\"></center>\n",
    "\n",
    " <br>\n",
    "    \n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/figs/boxes.gif\" width=\"600\"/>\n",
    "</div>\n",
    "</center>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center>Classification involves categorizing data into predefined classes or groups based on their features.</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d028f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Why do we use Machine Learning in Classification?</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be01fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages of Machine Learning for Classification\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Efficiency at Scale**: Machine learning algorithms can quickly classify large volumes of data with high accuracy.\n",
    "- **Pattern Recognition**: ML models excel at recognizing complex patterns in data that are not easily discernible by humans.\n",
    "- **Adaptability**: ML classifiers can adapt to new, unseen data, making them ideal for dynamic environments.\n",
    "- **Automation**: Automates the decision-making process in real-time applications, like spam detection or medical diagnoses.\n",
    "- **Continuous Improvement**: ML models can learn from new data over time, improving their accuracy and robustness.\n",
    "\n",
    "> Machine learning has transformed the landscape of classification, providing tools that offer precision, speed, and flexibility, which are unparalleled by traditional human and statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c342653",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Real-World Examples\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Business**: From customer segmentation to fraud detection.\n",
    "- **Healthcare**: Predicting disease outbreaks, patient diagnosis, and treatment planning.\n",
    "- **Technology**: Image and Voice recognition.\n",
    "- **Political Science**: Expressed Policy Agendas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f934da8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>What is Naive Bayes?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7de619",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Naive Bayes?\n",
    "\n",
    "<br>\n",
    "\n",
    "- A probabilistic machine learning model based on Bayes' Theorem.\n",
    "\n",
    "- Assumes independence among predictors.\n",
    "\n",
    "- Simple yet powerful for classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf97c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Origins of Bayes' Theorem\n",
    "\n",
    "<br>\n",
    "\n",
    "- Developed by the English statistician Thomas Bayes (1701–1761).\n",
    "\n",
    "- Bayes' Theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d0ae5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)\n",
    "\n",
    "- Developed by the English statistician Thomas Bayes (1701–1761). Bayes' Theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
    "\n",
    "- $P(A|B)$ is the **posterior probability**: Probability of event A occurring given that B is true - updated probability after the evidence is considered.\n",
    "- $P(A)$ is the **prior probability**: Initial probability of event A - the probability before the evidence is considered.\n",
    "- $P(B|A)$ is the **likelihood**: Probability of observing event B given that A is true.\n",
    "- $P(B)$ is the **marginal probability**: Total probability of the evidence, event B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d3237",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deriving Bayes' Theorem\n",
    "\n",
    "## From Conditional Probability to Bayes' Theorem <a class=\"tocSkip\">\n",
    "\n",
    "Given that the definition of Conditional Probability is:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$\n",
    "\n",
    "And knowing:\n",
    "\n",
    "$$ P(A \\cap B) = P(B|A) \\cdot P(A) $$\n",
    "\n",
    "We get:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
    "\n",
    "### <center>**This is Bayes' Theorem!** <a class=\"tocSkip\"> </center>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "Slide Navigation: <a href=\"#/20/1\">Link to Bayes' Theorem for Classification </a>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77453542",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Bayes' Theorem Matters?\n",
    "\n",
    "<br>\n",
    "\n",
    "Bayes' Theorem is a foundational principle in probability theory and statistics, enabling:\n",
    "\n",
    "- **Incorporaton of Prior Knowledge**: It allows for the integration of prior knowledge or beliefs when making statistical inferences.\n",
    "\n",
    "- **Beliefs Update**: It provides a systematic way to update the probability estimates as new evidence or data becomes available.\n",
    "\n",
    "- **Probabilistic Thinking**: Encourages a probabilistic approach to decision making, quantifying uncertainty, and reasoning under uncertainty.\n",
    "\n",
    "- **Versatility in Applications**: From medical diagnosis to spam filtering, Bayes' Theorem is pivotal in areas requiring probabilistic assessment.\n",
    "\n",
    "[Bayes' Theorem is a paradigm](https://www.sciencedirect.com/topics/mathematics/bayesian-paradigm#:~:text=Bayesian%20Methodology%20in%20Statistics&text=By%20using%20probability%20distributions%20to,coherence%20of%20the%20proposed%20solutions.) that shapes the way we interpret and interact with data, offering a powerful tool for learning from information and making decisions in an uncertain world. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fde70c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Medical Diagnosis\n",
    "\n",
    "\n",
    "### What is the probability that a person has the disease if they tested positive?\n",
    "\n",
    "<br>\n",
    "\n",
    "Consider a test for a particular disease, which has the following characteristics:\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Disease Prevalence (Prior Probability)**: 0.1%, $P(Disease) = 0.001$.\n",
    "\n",
    "\n",
    "- **Test Sensitivity (True Positive Rate)**: 99%, $P(Pos | Disease) = 0.99$.\n",
    "\n",
    "\n",
    "- **Test Specificity (True Negative Rate)**: 95%, $P(Neg | NoDisease) = 0.95$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09f6b7b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Understanding Our Priors\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Prior Probability of Having the Disease**: $P(Disease) = 0.001$\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Prior Probability of Not Having the Disease**: $P(NoDisease) = 1 - P(Disease) = 0.999$\n",
    "\n",
    "<br>\n",
    "\n",
    "These priors are essential for our Bayes' Theorem calculation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e2e513",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applying Bayes' Theorem\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "To find the posterior probability $P(Disease | Pos)$, we apply Bayes' Theorem:\n",
    "\n",
    "$$ P(Disease | Pos) = \\frac{P(Pos | Disease) \\cdot P(Disease)}{P(Pos)} $$\n",
    "\n",
    "<br>\n",
    "\n",
    "Where $P(Pos)$ can be found using the [law of total probability](https://en.wikipedia.org/wiki/Law_of_total_probability):\n",
    "\n",
    "$$ P(Pos) = P(Pos | Disease) \\cdot P(Disease) + P(Pos | NoDisease) \\cdot P(NoDisease) $$\n",
    "\n",
    "> The law of total probability is a fundamental rule relating marginal probabilities to conditional probabilities. It states that the probability of an event can be found by considering all possible ways that the event can occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9977e3e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Finding $P(Pos)$\n",
    "\n",
    "<br>\n",
    "\n",
    "Given our test specificity is 95%, the false positive rate is 5% ($P(Pos | NoDisease) = 0.05$).\n",
    "\n",
    "$$ P(Pos) = (0.99 \\times 0.001) + (0.05 \\times 0.999) $$\n",
    "$$ P(Pos) = 0.051 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a942fcef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Posterior Probability\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Given**: $P(Pos | Disease) = 0.99$, $P(Disease) = 0.001$, and our calculated $P(Pos) = 0.051$.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ P(Disease | Pos) = \\frac{0.99 \\times 0.001}{0.051 } $$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ P(Disease | Pos) = 0.019 $$\n",
    "\n",
    "Despite testing positive, **an individual has only a $1.9\\%$ chance of having the disease**. This result might seem counterintuitive given the high test sensitivity (99%), but it is largely due to the low prevalence of the disease (0.1%) and the impact of false positives in the wider population.\n",
    "\n",
    ">**Implications**: This underscores a crucial aspect of diagnostic tests - a high sensitivity rate does not guarantee a high probability of having the disease upon testing positive, especially when the disease prevalence is low. The relatively low posterior probability highlights the importance of considering both the characteristics of the test (such as sensitivity and specificity) and the prevalence of the condition in the population when interpreting test results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf89237",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Bayes' Theorem for Classification</center>\n",
    "\n",
    "<a id='another_cell'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89a7a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Theorem for Classification\n",
    "\n",
    "<br>\n",
    "\n",
    "Given a set of features $X = (x_1, x_2, ..., x_n)$, we want to predict the class $C_k$ out of $m$ possible classes.\n",
    "\n",
    "The goal is to find:\n",
    "\n",
    "$$ P(C_k|X) = \\frac{P(X|C_k) \\cdot P(C_k)}{P(X)} $$\n",
    "\n",
    "- **$P(C_k|X)$** is the **posterior probability**: Probability of class $C_k$ given features $X$.\n",
    "- **$P(C_k)$** is the **prior probability**: Probability of class $C_k$.\n",
    "- **$P(X|C_k)$** is the **likelihood**: Likelihood of features $X$ given class $C_k$.\n",
    "- **$P(X)$** is the **marginal probability**: Evidence, the total probability of observing features $X$.\n",
    "\n",
    "### How do we deal with $X$ being multidimensional? <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c64e21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Complexity of High Dimensionality\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- With **multidimensional feature vectors** $X = (x_1, x_2, ..., x_n)$, calculating the likelihood, $P(X|C_k)$, directly becomes impractical due to the **curse of dimensionality**.\n",
    "\n",
    "<br>\n",
    "\n",
    "- High-dimensional spaces increase the **data requirement exponentially**.\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Direct calculation** of $P(X|C_k)$ involves understanding complex relationships among all features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a90928",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: The Challenge of High Dimensionality in Spam Detection\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Context**: Email spam detection based on multidimensional feature vectors $X = (x_1, x_2, ..., x_n)$.\n",
    "\n",
    "- **Goal**: Classify emails into spam ($C_1$) or not spam ($C_2$).\n",
    "\n",
    "### Multidimensional Features <a class=\"tocSkip\">\n",
    "\n",
    "- Frequency of specific keywords (e.g., \"offer\", \"free\").\n",
    "- Email length.\n",
    "- Use of capital letters.\n",
    "- Presence of attachments.\n",
    "- Time of day the email was sent.\n",
    "\n",
    "Each feature contributes to identifying spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc9733",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: The Challenge of High Dimensionality in Spam Detection\n",
    "\n",
    "\n",
    "### Complexity of Direct Calculation $P(X|C_k)$ <a class=\"tocSkip\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- Directly calculating $P(X|C_k)$ requires assessing how all features $x_1, x_2, ..., x_n$ collectively influence the likelihood of an email being spam.\n",
    "\n",
    "<br>\n",
    "\n",
    "- With 10 binary features, there are $2^{10} = 1024$ possible combinations.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Accurately estimating $P(X|C_k)$ for all combinations requires a vast dataset, often impractical to obtain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79af113",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>The Naive Assumption</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35bb43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Naive Assumption\n",
    "\n",
    "The Naive Bayes assumption simplifies the problem by assuming **each feature $x_i$ is independent of every other feature**.\n",
    "\n",
    "By treating each feature as independent, we only need to calculate the probability of each feature individually given the class, rather than all possible combinations of features.\n",
    "\n",
    "This leads to:\n",
    "\n",
    "$$ P(X|C_k) = P(x_1, x_2, ..., x_n|C_k) = \\prod_{i=1}^{n} P(x_i|C_k) $$\n",
    "\n",
    "Thus, we can rewrite the Bayes' Theorem and our classifier becomes:\n",
    "\n",
    "$$ P(C_k|X) = \\frac{P(C_k) \\prod_{i=1}^{n} P(x_i|C_k)}{P(X)} $$\n",
    "\n",
    "- **Independence**: Each feature $x_i$ is independent of every other feature given the class $C_k$.\n",
    "- This assumption significantly reduces computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd049bbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>The Naive Bayes Classifier</center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305e728",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Naive Bayes Classifier \n",
    "\n",
    "\n",
    "**Classification Decision**: Since $P(X)=P(x_1, \\dots, x_n) $  is constant across all classes, we focus on the classification rule that maximize the numerator:\n",
    "\n",
    "$$\n",
    "P(C_k \\mid x_1, \\dots, x_n) \\propto P(C_k) \\prod_{i=1}^{n} P(x_i \\mid C_k)\n",
    "$$\n",
    "$$\n",
    "\\Downarrow\n",
    "$$\n",
    "\n",
    "$$ \\hat{C} = \\arg \\max_{C_k} P(C_k) \\prod_{i=1}^{n} P(x_i|C_k) $$\n",
    "\n",
    "$ P(C_k) $ is then the relative frequency of class $ C $\n",
    "in the training set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5328191",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Statistical Learning Models</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f7933e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "Models are trained on labeled data. Each training example includes an input and a corresponding output label. The goal is to learn a mapping function from inputs to outputs to make predictions on new, unseen data.\n",
    "\n",
    "### Examples <a class=\"tocSkip\">\n",
    "\n",
    "- **Linear Regression**: Predicts a continuous output. Common in predicting housing prices, stock market trends, etc.\n",
    "- **Naive Bayes Classifiers**: Suitable for classification tasks like: image recognition, spam detection, and document categorization.\n",
    "- **Random Forests**: Versatile for classification and regression tasks. Used in customer segmentation, medical diagnosis, and more.\n",
    "- **Support Vector Machines (SVM)**: Effective in high-dimensional spaces for classification tasks such as image recognition and text categorization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75defce2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "Models work with unlabeled data, focusing on identifying patterns, clusters, or relationships within the data without predefined labels.\n",
    "\n",
    "### Examples <a class=\"tocSkip\">\n",
    "\n",
    "- **K-Means Clustering**: Identifies clusters within the data. Applied in market segmentation, gene sequence analysis, etc.\n",
    "- **Principal Component Analysis (PCA)**: A dimensionality reduction technique used to reduce the dimensionality of large data sets.\n",
    "- **Topic Models (e.g., Latent Dirichlet Allocation - LDA)**: Used to discover the abstract \"topics\" that occur in a collection of documents. Topic modeling is widely used in text mining for uncovering hidden thematic structures in text data, such as finding trends in scientific literature, organizing large archives of documents, or enhancing search engines.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f19695",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive Bayes as a Generative Model\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Naive Bayes** is a fundamental example of a generative model.\n",
    "\n",
    "- Uses Bayes' Theorem to estimate the probability of each class given the feature vector $X$.\n",
    "\n",
    "$$ P(C_k|X) = \\frac{P(X|C_k)P(C_k)}{P(X)} $$\n",
    "\n",
    "- By learning the distribution of each class, Naive Bayes models the generation process of the data.\n",
    "\n",
    "- This allows not only for classification but also for generating new data samples based on the learned distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f2db3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Naive Bayes Classifiers</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d496c2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "- **Assumption**: The features $x_i$ are assumed to be normally distributed (Gaussian) for each class $C_k$.\n",
    "- **Applicability**: Ideal for datasets where features are continuous and can be approximated by a Gaussian distribution.\n",
    "- **`Python`**: `GaussianNB` in the `scikit-learn` package implements the Gaussian Naive Bayes algorithm for classification.\n",
    "\n",
    "\n",
    "$$ P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k}^{2}}} \\exp\\left(-\\frac{(x_i - \\mu_{k})^2}{2\\sigma_{k}^{2}}\\right) $$\n",
    "\n",
    "- Where $\\mu_{k}$ and $\\sigma_{k}^{2}$ are the mean and variance of feature $x_i$ for class $C_k$. They are estimated using [maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#:~:text=In%20statistics%2C%20maximum%20likelihood%20estimation,observed%20data%20is%20most%20probable.).\n",
    "\n",
    "Slide Navigation: <a href=\"#/44/1\">Link to Practice Menu </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d4c3e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "- **Assumption**: The features $x_i$ represent the frequencies with which certain events have been generated by a multinomial distribution.\n",
    "- **Applicability**: Suited for count data, such as the frequency of words in text documents.\n",
    "- **`Python`**: `MultinomialNB` in the `scikit-learn` package implements the naive Bayes algorithm. The distribution is parametrized by vectors $ \\theta_{C_k} = (\\theta_{C_k1},\\ldots,\\theta_{C_kn}) $\n",
    "for each class $ C_k $, where $ n $ is the number of features and $ \\theta_{C_ki} $ is the probability $ P(x_i \\mid C_k) $ of feature $ i $ appearing in a sample belonging to class $ C_k $.\n",
    "\n",
    "The parameters $ \\theta_{C_k} $ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{C_ki} = \\frac{ N_{C_ki} + \\alpha}{N_{C_k} + \\alpha n}\n",
    "$$\n",
    "\n",
    "where $ N_{C_ki} = \\sum_{x \\in T} x_i $ is the number of times feature $ i $ appears in a sample of class $ C_k $\n",
    "in the training set $ T $, and $ N_{C_k} = \\sum_{i=1}^{n} N_{C_ki} $ is the total count of\n",
    "all features for class $ C_k $.\n",
    "\n",
    "If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, because the probability estimate is directly proportional to the number of occurrences of a feature's value. The smoothing priors $ \\alpha \\ge 0 $ accounts for features not present in the learning samples (*pseudocount*) and prevents zero probabilities in further computations. Setting $ \\alpha = 1 $ is called [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), while $ \\alpha < 1 $ is called [Lidstone smoothing](https://en.wikipedia.org/wiki/Additive_smoothing).\n",
    "\n",
    "<br>\n",
    "\n",
    "Slide Navigation: <a href=\"#/44/1\">Link to Practice Menu </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf254ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bernoulli Naive Bayes\n",
    "\n",
    "- **Assumption**: The features $x_i$ are binary (Boolean) variables indicating the presence or absence of a feature.\n",
    "\n",
    "- **Applicability**: Effective for datasets where features are binary, such as text classification where a word's presence or absence is a feature.\n",
    "\n",
    "- **`Python`**: `BernoulliNB` in the `scikit-learn` package implements the Naive Bayes for data that is distributed according to multivariate Bernoulli distributions.\n",
    "\n",
    "The likelihood for Bernoulli Naive Bayes is\n",
    "\n",
    "$$\n",
    "P(x_i \\mid C_k) = P(x_i = 1 \\mid C_k) x_i + (1 - P(x_i = 1 \\mid C_k)) (1 - x_i)\n",
    "$$\n",
    "\n",
    "which differs from multinomial NB’s rule in that it explicitly penalizes the non-occurrence of a feature $ i $, where the multinomial variant would simply ignore a non-occurring feature.\n",
    "\n",
    "<br>\n",
    "\n",
    "Slide Navigation: <a href=\"#/44/1\">Link to Practice Menu </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd16cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing the Right Naive Bayes Classifier\n",
    "\n",
    "The best classifier aligns with the statistical properties of your data and performs best empirically.\n",
    "    \n",
    "## Selection Strategy <a class=\"tocSkip\">\n",
    "\n",
    "1. **Analyze Features**: Understand the distribution of your data (plot your data!).\n",
    "2. **Preprocess**: Tailor preprocessing to fit the model's assumptions (e.g. log transformations).\n",
    "3. **Domain Knowledge**: Let insights from the domain guide your choice.\n",
    "\n",
    "4. **Model Comparison**: Apply different models and evaluate their performance.\n",
    "5. **Hyperparameter Tuning**: For Naive Bayes models, particularly Multinomial and Bernoulli, the `alpha` parameter is crucial. Correctly tuning the `alpha` parameter is a form of regularization that enhances the model's accuracy and robustness, especially important for sparse or imbalanced datasets.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f4c2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring Performance\n",
    "\n",
    "**Confusion Matrix**: it is a powerful tool for measuring the performance of a classification model. It provides insights beyond overall accuracy, allowing for a detailed analysis of the model's effectiveness.\n",
    "\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85edb6bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\dfrac{\\text{correct predictions}}{\\text{total observations}} \\ = \\ \\dfrac{tp + tn}{tp + tn + fp + fn}$$\n",
    "\n",
    "- Overall effectiveness of the model.\n",
    "- High accuracy: lots of correct predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a4b8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision\n",
    "\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "<br>\n",
    "    \n",
    "$$\\dfrac{\\text{true positives}}{\\text{total predicted positive}} = \\dfrac{tp}{tp + fp}$$\n",
    "\n",
    "- Accuracy of positive predictions.\n",
    "- High precision: low false-positive rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa64817",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall\n",
    "\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\dfrac{\\text{true positives}}{\\text{total actual positive}} \\ = \\ \\dfrac{tp}{tp + fn}$$\n",
    "\n",
    "\n",
    "- Is the fraction of positives correctly identified.\n",
    "- High recall: low false-negative rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60387f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## F1-Score\n",
    "\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "$$ \\text{F1} \\ = \\ 2 \\times \\dfrac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} $$\n",
    "\n",
    "- Harmonic mean of Precision and Recall.\n",
    "- A higher F1 score indicates a better balance between precision and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cac5136",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Understanding Model Performance\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Imbalanced Classes**: Precision, Recall, and F1 score are crucial in the presence of imbalanced classes.\n",
    "\n",
    "- **Model Objective**: Prioritize metrics based on the application needs, e.g., Recall over Precision in medical diagnostics.\n",
    "\n",
    "The **confusion matrix** offers a comprehensive way to evaluate and improve classification models by providing insights into their specific strengths and weaknesses.\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c961d2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Let's Practice!</center>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Slide Navigation: <a href=\"#/45/0\">Link to Gaussian Naive Bayes</a>\n",
    "\n",
    "- Slide Navigation: <a href=\"#/82/0\">Link to Multinomial Naive Bayes</a>\n",
    "\n",
    "- Slide Navigation: <a href=\"#/104/1\">Link to Bernoulli Naive Bayes</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceba1e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice: Gaussian Naive Bayes\n",
    "\n",
    "## Pima Indians Diabetes Database Overview <a class=\"tocSkip\">\n",
    "\n",
    "### Context <a class=\"tocSkip\">\n",
    "\n",
    "- This dataset ([Kaggle Dataset Link](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data)) originates from the **National Institute of Diabetes and Digestive and Kidney Diseases**. \n",
    "- All patients are females of at least 21 years old of **Pima Indian heritage**, selected under specific constraints for this study.\n",
    "- **Objective**: The aim is to predict whether a patient has diabetes based on diagnostic measurements. \n",
    "\n",
    "### Assumption <a class=\"tocSkip\">\n",
    "    \n",
    "- The features $x_i$ can be aproximated by a Gaussian distribution.    \n",
    "\n",
    "<br>\n",
    "    \n",
    "Slide Navigation: <a href=\"#/44/1\">Link to Practice Menu </a>\n",
    "    \n",
    "\n",
    "Slide Navigation: <a href=\"#/129/1\">Link to Audience Questions </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a88bfc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Content <a class=\"tocSkip\">\n",
    "\n",
    "The dataset features several medical predictor variables alongside one target variable, `Outcome`. Predictor variables include:\n",
    "\n",
    "- Number of pregnancies\n",
    "- BMI (Body Mass Index)\n",
    "- Insulin level\n",
    "- Age\n",
    "- ...among others.\n",
    "\n",
    "These variables assist in diagnostically predicting diabetes presence.\n",
    "    \n",
    "### Assumption <a class=\"tocSkip\">\n",
    "\n",
    "- The features $x_i$ are assumed to be normally distributed (Gaussian) for each class $C_k$.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8281e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Import libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67732bf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361a295",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ee493",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Preview data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd32c17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset dimensions - (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff8d10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Features data-type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e8a8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ee924",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Count of null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e0b2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Observations: <a class=\"tocSkip\">\n",
    "\n",
    "1. There are a total of 768 records and 9 features in the dataset.\n",
    "2. Each feature can be either of integer or float dataype.\n",
    "3. Some features like Glucose, Blood pressure, Insulin, BMI have zero values which represent missing data.\n",
    "4. There are zero NaN values in the dataset.\n",
    "5. In the outcome column, 1 represents diabetes positive and 0 represents diabetes negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26eba96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Outcome countplot\n",
    "sns.countplot(x = 'Outcome',data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d34d43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Histogram of each feature\n",
    "\n",
    "col = df.columns[:8]\n",
    "plt.subplots(figsize = (20, 15))\n",
    "length = len(col)\n",
    "\n",
    "for i, j in itertools.zip_longest(col, range(length)):\n",
    "    plt.subplot((length//2), 3, j + 1)\n",
    "    plt.subplots_adjust(wspace = 0.1,hspace = 0.5)\n",
    "    df[i].hist(bins = 20)\n",
    "    plt.title(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494a0b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Scatter plot matrix \n",
    "scatter_matrix(df, figsize = (20, 20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694147e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Pairplot \n",
    "sns.pairplot(data = df, hue = 'Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f05180",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "sns.heatmap(df.corr(), annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280b439",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Observations: <a class=\"tocSkip\">\n",
    "\n",
    "1. The countplot tells us that the dataset is imbalanced, as number of patients who don't have diabetes is more than those who do.\n",
    "\n",
    "2. From the correaltion heatmap, we can see that there is a high correlation between Outcome and [Glucose,BMI,Age,Insulin]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c40a7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da32c6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_new = df\n",
    "# list(df_new)\n",
    "# Preview data\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71363cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# checking zero values\n",
    "\n",
    "# np.where(df_new['Glucose'] == 0)[0].shape\n",
    "# np.where(df_new['BloodPressure'] == 0)[0].shape\n",
    "np.where(df_new['SkinThickness'] == 0)[0].shape\n",
    "# np.where(df_new['Insulin'] == 0)[0].shape\n",
    "# np.where(df_new['BMI'] == 0)[0].shape\n",
    "# np.where(df_new['DiabetesPedigreeFunction'] == 0)[0].shape\n",
    "# np.where(df_new['Age'] == 0)[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cdd3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing zero values with NaN\n",
    "df_new[['Glucose',\n",
    " 'BloodPressure',\n",
    " 'SkinThickness',\n",
    " 'Insulin',\n",
    " 'BMI',\n",
    " 'DiabetesPedigreeFunction',\n",
    " 'Age']] = df_new[['Glucose',\n",
    " 'BloodPressure',\n",
    " 'SkinThickness',\n",
    " 'Insulin',\n",
    " 'BMI',\n",
    " 'DiabetesPedigreeFunction',\n",
    " 'Age']].replace(0, np.NaN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081c2e4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Count of NaN\n",
    "df_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d3cb72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Removing Features with too many zeros NaNs\n",
    "df_new = df_new.drop(['SkinThickness', 'Insulin'], axis = 1)\n",
    "\n",
    "# Removing Observations with NaNs\n",
    "df_new = df_new.dropna(subset=['Glucose'])\n",
    "df_new = df_new.dropna(subset=['BloodPressure'])\n",
    "df_new = df_new.dropna(subset=['BMI'])\n",
    "\n",
    "#dataset_new = dataset_new[dataset_new['Glucose'] != 0]\n",
    "#dataset_new = dataset_new[dataset_new['BloodPressure'] != 0]\n",
    "#dataset_new = dataset_new[dataset_new['BMI'] != 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdddbd1a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df_new.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee4e8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# log transformation\n",
    "df_new['LogPregnancies'] = np.log1p(df_new['Pregnancies'])\n",
    "df_new['LogDiabetesPedigreeFunction'] = np.log1p(df_new['DiabetesPedigreeFunction'])\n",
    "df_new['LogAge'] = np.log1p(df_new['Age'])\n",
    "\n",
    "# Statistical summary\n",
    "df_new.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a5324",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Histogram of each feature\n",
    "\n",
    "col = df_new.columns[:10]\n",
    "plt.subplots(figsize = (20, 15))\n",
    "length = len(col)\n",
    "\n",
    "for i, j in itertools.zip_longest(col, range(length)):\n",
    "    plt.subplot((length//2), 3, j + 1)\n",
    "    plt.subplots_adjust(wspace = 0.1,hspace = 0.5)\n",
    "    df_new[i].hist(bins = 20)\n",
    "    plt.title(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440008c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Selecting features \n",
    "features = [\n",
    "    'LogPregnancies',\n",
    "    'Glucose',\n",
    "    'BloodPressure',\n",
    "    'BMI',\n",
    "    'LogDiabetesPedigreeFunction',\n",
    "    'LogAge'\n",
    "]\n",
    "\n",
    "# Splitting X and Y\n",
    "df_train, df_test = train_test_split(df_new, test_size = 0.20, random_state = 42, stratify = df_new['Outcome'] )\n",
    "\n",
    "X_train = df_train[features]\n",
    "Y_train = df_train['Outcome']\n",
    "X_test = df_test[features]\n",
    "Y_test = df_test['Outcome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8c802",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Checking dimensions\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20b005",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecfda4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Algorithm\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0afba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Making predictions on test dataset\n",
    "\n",
    "Y_pred_nb = nb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bd1f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d4a02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluating using accuracy_score metric\n",
    "\n",
    "accuracy_nb = accuracy_score(Y_test, Y_pred_nb)\n",
    "\n",
    "# Accuracy on test set\n",
    "print(\"Naive Bayes: \" + str(accuracy_nb * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad454c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred_nb)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60002e02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Heatmap of Confusion matrix\n",
    "sns.heatmap(pd.DataFrame(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341f836",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Classification report\n",
    "\n",
    "print(classification_report(Y_test, Y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db30139",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "X = df_new[features]\n",
    "y = df_new['Outcome']\n",
    "result = cross_val_score(nb, X, y, scoring = 'accuracy')\n",
    "# Recall: If we consider that the cost of not classifying someone with diabetes is high, \n",
    "# that failing to identify a sick patient (a false negative) is more dangerous \n",
    "# than incorrectly diagnosing a healthy patient as sick (a false positive).\n",
    "\n",
    "result.mean(), result.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e90706",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice: Multinomial Naive Bayes\n",
    "\n",
    "### BBC Full Text Document Classification <a class=\"tocSkip\">\n",
    "\n",
    "### Context <a class=\"tocSkip\">\n",
    "\n",
    "- The original dataset ([Kaggle Dataset](https://www.kaggle.com/datasets/dheemanthbhat/bbc-full-text-preprocessed)) \n",
    "consists of 2225 documents (as text files) from the BBC news website corresponding to news articles in five topical areas: \n",
    "    - business\n",
    "    - entertainment \n",
    "    - politics\n",
    "    - sport\n",
    "    - tech\n",
    "\n",
    "- **Objective**: The aim is to predict which topic does a news article belong to based on its content.\n",
    "\n",
    "### Assumption <a class=\"tocSkip\">\n",
    "    \n",
    "- The features $x_i$ represent the frequencies with which certain events have been generated by a multinomial distribution.    \n",
    "\n",
    "<br>\n",
    "    \n",
    "Slide Navigation: <a href=\"#/44/1\">Link to Practice Menu </a>\n",
    "\n",
    "Slide Navigation: <a href=\"#/129/1\">Link to Audience Questions </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759c9f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f216f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "train_df: pd.DataFrame = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/mnist_train_subset.csv')\n",
    "test_df: pd.DataFrame = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/mnist_test.csv')\n",
    "\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/bbc_text_cls.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97500f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Preview data\n",
    "df.head()\n",
    "\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae01cb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680bbc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's check labels frequency\n",
    "\n",
    "df['labels'].hist(figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601556d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0f81d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Handles the removal of stopwords and stemming\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and make lower case\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stem the words\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fbce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284615f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the preprocessing to each row\n",
    "\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf96799",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de86d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "features = df['processed_text']\n",
    "labels = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed721a14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad4251",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a text processing and classification pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    # Bag of Words\n",
    "    # Convert the processed text into a matrix of token counts, \n",
    "    # which is then used as input to the MultinomialNB classifier\n",
    "    CountVectorizer(),\n",
    "    MultinomialNB()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ace1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b3cb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "pipeline.fit(features_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb6ee7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "labels_pred = pipeline.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d9104",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3673325",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model using train-test split\n",
    "print(\"Train-test split evaluation:\")\n",
    "print(classification_report(labels_test, labels_pred))\n",
    "print(f\"Accuracy: {accuracy_score(labels_test, labels_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7017d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model using cross-validation\n",
    "\n",
    "cross_val_accuracy = cross_val_score(pipeline, features_train, labels_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"\\nCross-validation evaluation:\")\n",
    "print(f\"Cross-validated accuracy: {np.mean(cross_val_accuracy)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d56b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix Display\n",
    "ConfusionMatrixDisplay.from_predictions(labels_test, labels_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47e0e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's Check Some Misclassified Examples\n",
    "\n",
    "# identifying misclassified examples\n",
    "misclassified_idx = np.where(labels_pred != labels_test)[0]\n",
    "\n",
    "# random select a misclassified example\n",
    "i = np.random.choice(misclassified_idx)\n",
    "\n",
    "print(\"True class:\", labels_test.iloc[i])\n",
    "print(\"Predicted class:\", labels_pred[i])\n",
    "\n",
    "# The specific element from 'features_test'\n",
    "specific_element = features_test.iloc[i]\n",
    "\n",
    "# Find the indices where the 'text' column in the DataFrame matches the specific element\n",
    "matching_indices = df.index[df['processed_text'] == specific_element].tolist()\n",
    "\n",
    "# Print text\n",
    "list(df.iloc[matching_indices,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95edd7a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the feature names and the log probability of features given a class\n",
    "feature_names = pipeline.named_steps['countvectorizer'].get_feature_names_out()\n",
    "feature_log_prob = pipeline.named_steps['multinomialnb'].feature_log_prob_\n",
    "\n",
    "# Create a DataFrame to hold the top words for each category\n",
    "top_words_per_category = pd.DataFrame()\n",
    "\n",
    "for i, category in enumerate(pipeline.named_steps['multinomialnb'].classes_):\n",
    "    # Get the indices of the top 10 features for this class\n",
    "    top_indices = np.argsort(feature_log_prob[i])[-10:]\n",
    "    # Get the associated words and probabilities\n",
    "    top_features = feature_names[top_indices]\n",
    "    top_probabilities = np.exp(feature_log_prob[i][top_indices])\n",
    "    # Add to the DataFrame\n",
    "    top_words_per_category[category] = top_features[::-1]  # Reverse to show the top word at first\n",
    "\n",
    "# Transpose the DataFrame to have categories as columns and words as rows\n",
    "top_words_per_category = top_words_per_category.T\n",
    "top_words_per_category.columns = [f'Top {i+1}' for i in range(top_words_per_category.shape[1])]\n",
    "\n",
    "# Print the table\n",
    "print(top_words_per_category.transpose())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa5515",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice: Bernoullil Naive Bayes\n",
    "\n",
    "### Context <a class=\"tocSkip\">\n",
    "\n",
    "- The original [Kaggle Dataset](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv) contains the 60,000 training examples and labels in addition to 10,000 test examples and labels. Each row consists of 785 values: the first value is the label (a number from 0 to 9) and the remaining 784 values are the pixel values (a number from 0 to 255).\n",
    "    \n",
    "- For our purposes in the lecture, 33,000 random training examples were removed from the original dataset.\n",
    "    \n",
    "- **Objective**: The aim is to predict the correct digit based on the hand-written image.\n",
    "    \n",
    "### Assumption <a class=\"tocSkip\">\n",
    "    \n",
    "- The features $x_i$ are binary (Boolean) variables indicating the presence or absence of a feature.\n",
    "    \n",
    "    \n",
    "<br>\n",
    "    \n",
    "Slide Navigation: <a href=\"#/44/1\">Link to Practice Menu </a>\n",
    "    \n",
    "\n",
    "Slide Navigation: <a href=\"#/129/1\">Link to Audience Questions </a>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30772e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Import libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c8b5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d139166",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df: pd.DataFrame = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/mnist_train_subset.csv')\n",
    "test_df: pd.DataFrame = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/mnist_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fc472",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662a169",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c894103",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e83f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c32164",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dface53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Sort the label counts by the label value, assuming they are categorical but not numerical\n",
    "label_counts = train_df['label'].value_counts().sort_index()\n",
    "\n",
    "# Create a bar plot for the label counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Labels in the Dataset')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=1)  # Rotate x labels for better readability if necessary\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c361bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3c960",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create variables for the pixels and the labels we want to predict\n",
    "X_train: np.ndarray = train_df.drop('label', axis=1).to_numpy()\n",
    "y_train: np.ndarray = train_df['label'].to_numpy()\n",
    "X_test: np.ndarray = test_df.drop('label', axis=1).to_numpy()\n",
    "y_test: np.ndarray = test_df['label'].to_numpy()\n",
    "\n",
    "#X_test: np.ndarray = test_df.to_numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ec1eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#print(X_train.ndim)\n",
    "#print(X_train.shape)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbe73f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3813de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a sample of each digit as the original image\n",
    "# create sub plot for each digit\n",
    "fig, ax = plt.subplots(2,5, figsize=(7,4))\n",
    "# loop over each subplot to add its digit\n",
    "for i, ax in enumerate(ax.flatten()):\n",
    "    # find index for image with the corresponding digit\n",
    "    img_idx: int = np.argwhere(y_train == i)[0]\n",
    "    # get the image and reshape to 28X28\n",
    "    img: np.ndarray = np.reshape(X_train[img_idx], (28, 28))\n",
    "    # plot digit image\n",
    "    ax.imshow(img, cmap=\"gray_r\")\n",
    "    # add digit label\n",
    "    ax.set_title(f\"Label: {i}\")\n",
    "    # remove gridlines\n",
    "    ax.grid(False)\n",
    "# add title to the plot\n",
    "fig.suptitle(\"MNIST Images Sample And Their Labels\")\n",
    "# adjust the padding between and around subplots\n",
    "fig.tight_layout()\n",
    "# show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535e97e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We assume each pixel is either 0 (black) or 1 (white)\n",
    "# create empty list for our binary vector\n",
    "X_train_binary: List = []\n",
    "# loop over each vector\n",
    "for i in X_train:\n",
    "    # reshape digit vector to image\n",
    "    img: np.ndarray = np.reshape(i, (28, 28)).astype(np.uint8)\n",
    "    # binarize\n",
    "    im_gray: np.ndarray = cv2.threshold(\n",
    "        img, 120, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    # append to binary vector list\n",
    "    X_train_binary.append(np.reshape(im_gray, (784,)))\n",
    "# convert to numpy array\n",
    "X_train_binary: np.ndarray = np.asarray(X_train_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bed3b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17bc40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = BernoulliNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea21b0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2ebc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "print(\"train acc:\", model.score(X_train, y_train))\n",
    "print(\"test acc:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6962b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the labels for the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Print the classification report for the test set\n",
    "print(\"Classification report for the test set:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e57c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the confusion matrix for the test set\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Display the confusion matrix using ConfusionMatrixDisplay\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=model.classes_)\n",
    "cmd.plot(values_format='d')\n",
    "plt.title('Confusion Matrix for the Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c415137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation with 5 folds\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-validation scores for each fold:\", cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00c715",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "mean_cv_score = cv_scores.mean()\n",
    "std_cv_score = cv_scores.std()\n",
    "\n",
    "# Print the mean and standard deviation\n",
    "print(f\"Mean cross-validation accuracy: {mean_cv_score:.3f}\")\n",
    "print(f\"Standard deviation of cross-validation accuracy: {std_cv_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13076a67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show some misclassified examples\n",
    "\n",
    "#np.random.seed(1)\n",
    "\n",
    "misclassified_idx = np.where(y_test_pred != y_test)[0]\n",
    "i = np.random.choice(misclassified_idx)\n",
    "plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"True label: {y_test[i]} Predicted: {y_test_pred[i]}\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f722f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Questions?</center> \n",
    "\n",
    "<br>\n",
    "    \n",
    "Slide Navigation: <a href=\"#/44/1\">Link to Practice Menu </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c7347",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework Assignment: Enhancing Naive Bayes Classifier Models\n",
    "\n",
    "## It is your turn! \n",
    "\n",
    "In this assignment, you are tasked with enhancing the predictive performance of one of the Naive Bayes classifier models we developed in class. This is an opportunity for you to implement and experiment with the machine learning concepts and techniques discussed during the lecture.\n",
    "\n",
    "### Objectives <a class=\"tocSkip\">\n",
    "\n",
    "- **Model Optimization**: Optimize the existing Naive Bayes classifier or select an alternative model that you believe could yield better results.\n",
    "- **Data Processing**: Apply different data preprocessing techniques to improve model accuracy.\n",
    "- **Research**: Conduct research to explore various strategies that could enhance model performance. Utilize reputable resources to support your choices.\n",
    "\n",
    "### Deliverables <a class=\"tocSkip\">\n",
    "\n",
    "1. **Enhanced Model Implementation**: A Python script or Jupyter Notebook containing the code for your improved model.\n",
    "2. **Performance Comparison**: A report comparing the original model's performance with your enhanced model. Include metrics such as accuracy, precision, recall, and F1-score.\n",
    "3. **Justification**: A detailed explanation of the changes you made, including:\n",
    "   - Why you chose to adjust or change the model.\n",
    "   - The data processing techniques you applied.\n",
    "   - Any resources or references you utilized to inform your decisions.\n",
    "\n",
    "### Evaluation Criteria <a class=\"tocSkip\">\n",
    "\n",
    "- **Innovation**: Creative and effective approaches to model enhancement.\n",
    "- **Accuracy**: The predictive performance of your final model.\n",
    "- **Justification**: The rationale behind your methodological choices.\n",
    "- **Presentation**: Clarity and structure of your comparative analysis and justifications.\n",
    "\n",
    "Your assignment will not only be evaluated on the improved accuracy of the model but also on your analytical approach and the ability to articulate your decision-making process.\n",
    "\n",
    "## <center>Have Fun!</center> <a class=\"tocSkip\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b71b94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Thank you!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b13b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "<br>\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An Introduction to Statistical Learning: With Applications in Python (1st ed. 2023 edition). Springer.\n",
    "\n",
    "- H. Zhang (2004). [The optimality of Naive Bayes.](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)\n",
    "  Proc. FLAIRS.  \n",
    "\n",
    "- C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to\n",
    "  Information Retrieval. Cambridge University Press, pp. 234-265.  \n",
    "\n",
    "- A. McCallum and K. Nigam (1998).\n",
    "  [A comparison of event models for Naive Bayes text classification.](https://citeseerx.ist.psu.edu/doc_view/pid/04ce064505b1635583fa0d9cc07cac7e9ea993cc)\n",
    "  Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.  \n",
    "\n",
    "- V. Metsis, I. Androutsopoulos and G. Paliouras (2006).\n",
    "  [Spam filtering with Naive Bayes – Which Naive Bayes?](https://citeseerx.ist.psu.edu/doc_view/pid/8bd0934b366b539ec95e683ae39f8abb29ccc757)\n",
    "  3rd Conf. on Email and Anti-Spam (CEAS).  \n",
    "\n",
    "- Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\n",
    "  [Tackling the poor assumptions of naive bayes text classifiers.](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf)\n",
    "  In ICML (Vol. 3, pp. 616-623).  \n",
    "\n",
    "- C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to\n",
    "  Information Retrieval. Cambridge University Press, pp. 234-265.  \n",
    "\n",
    "- A. McCallum and K. Nigam (1998).\n",
    "  [A comparison of event models for Naive Bayes text classification.](https://citeseerx.ist.psu.edu/doc_view/pid/04ce064505b1635583fa0d9cc07cac7e9ea993cc)\n",
    "  Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.  \n",
    "\n",
    "- V. Metsis, I. Androutsopoulos and G. Paliouras (2006).\n",
    "  [Spam filtering with Naive Bayes – Which Naive Bayes?](https://citeseerx.ist.psu.edu/doc_view/pid/8bd0934b366b539ec95e683ae39f8abb29ccc757)\n",
    "  3rd Conf. on Email and Anti-Spam (CEAS).  \n",
    "\n",
    "- Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\n",
    "  [Tackling the poor assumptions of naive bayes text classifiers.](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf)\n",
    "  In ICML (Vol. 3, pp. 616-623).  \n",
    "\n",
    "- Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). *Using the ADAP learning algorithm to forecast the onset of diabetes mellitus*. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.\n",
    "- D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006.\n",
    "  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1706230122.46583,
  "filename": "naive_bayes.rst",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "scroll": true
  },
  "title": "Naive Bayes",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1271px",
    "left": "131.969px",
    "top": "168.938px",
    "width": "354.344px"
   },
   "toc_section_display": false,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
