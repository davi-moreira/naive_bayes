{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454e37d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/figs/bayes_theorem.png\" width=\"400\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "# <center>Naive Bayes Classifiers <a class=\"tocSkip\"></center>\n",
    "## <center>Davi Moreira <a class=\"tocSkip\"></center>\n",
    "### <center>Quantitative Theory and Methods <a class=\"tocSkip\"></center>\n",
    "### <center>Emory University <a class=\"tocSkip\"></center>\n",
    "    \n",
    "<!---Notebook Navigation: [Another Cell](#another_cell)\n",
    "Slide Navigation: <a href=\"#/2/3\">Link to xxx</a>--->\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3dd72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/figs/naivebayesshort.png\" width=\"400\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "## <center> [http://tiny.cc/naive-bayes](http://tiny.cc/naive-bayes) <a class=\"tocSkip\"></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "# <center> Access the link or the QR code to follow the material <a class=\"tocSkip\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3ca39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today's Summary and Learning Objectives\n",
    "\n",
    "1. **Statistical Foundations** of Naive Bayes Classifiers.\n",
    "2. The beauty of Bayes Theorem in action;\n",
    "3. The Naive Bayes assumptions;\n",
    "4. Naive Bayes Classifiers **Implementation with `Python`**.\n",
    "5. Three versions of Naive Bayes Classifiers:\n",
    "                - Gaussian Naive Bayes.\n",
    "                - Multinomial Naive Bayes.\n",
    "                - Bernoulli Naive Bayes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5724caf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>What is a classification problem?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf289d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>What is a classification problem? <a class=\"tocSkip\"></center>\n",
    "\n",
    " <br>\n",
    "    \n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/figs/boxes.gif\" width=\"600\"/>\n",
    "</div>\n",
    "</center>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center>Classification involves categorizing data into predefined classes or groups based on their features.</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d028f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Why do we use Machine Learning in Classification?</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9be01fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages of Machine Learning for Classification\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Efficiency at Scale**: Machine learning algorithms can quickly classify large volumes of data with high accuracy.\n",
    "- **Pattern Recognition**: ML models excel at recognizing complex patterns in data that are not easily discernible by humans.\n",
    "- **Adaptability**: ML classifiers can adapt to new, unseen data, making them ideal for dynamic environments.\n",
    "- **Automation**: Automates the decision-making process in real-time applications (e.g. spam detection).\n",
    "- **Continuous Improvement**: ML models can learn from new data over time, improving their accuracy and robustness.\n",
    "\n",
    "> Machine learning has transformed the landscape of classification, providing tools that offer precision, speed, and flexibility, which are unparalleled by traditional human and statistical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f934da8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Where does the Naive Bayes' classifier come from?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d0ae5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)\n",
    "\n",
    "- Developed by the English statistician Thomas Bayes (1701â€“1761). It describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
    "\n",
    "- $P(A|B)$ is the **posterior probability**: Probability of event A occurring given that B is true - updated probability after the evidence is considered.\n",
    "- $P(A)$ is the **prior probability**: Initial probability of event A - the probability before the evidence is considered.\n",
    "- $P(B|A)$ is the **likelihood**: Probability of observing event B given that A is true.\n",
    "- $P(B)$ is the **marginal probability**: Total probability of the evidence, event B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d3237",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deriving Bayes' Theorem\n",
    "\n",
    "## From Conditional Probability to Bayes' Theorem <a class=\"tocSkip\">\n",
    "\n",
    "Given that the definition of Conditional Probability is:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$\n",
    "\n",
    "And knowing:\n",
    "\n",
    "$$ P(A \\cap B) = P(A|B) \\cdot P(B) $$\n",
    "$$ P(B \\cap A) = P(B|A) \\cdot P(A) $$\n",
    "\n",
    "Joint probability is symmetric, meaning:\n",
    "\n",
    "$$ P(A \\cap B) = P(B \\cap A) $$\n",
    "\n",
    "Thus, we can also express it as:\n",
    "\n",
    "$$ P(A \\cap B) = P(B|A) \\cdot P(A) $$\n",
    "\n",
    "We get:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
    "\n",
    "### <center>**The Bayes' Theorem!** <a class=\"tocSkip\"> </center>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf89237",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>How do we use the Bayes' Theorem for Classification?</center>\n",
    "\n",
    "<a id='another_cell'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb89a7a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes' Theorem for Classification\n",
    "\n",
    "<br>\n",
    "\n",
    "Given a set of features $X = (x_1, x_2, ..., x_n)$, we want to predict the class $C_k$ out of $m$ possible classes.\n",
    "\n",
    "The goal is to find:\n",
    "\n",
    "$$ P(C_k|X) = \\frac{P(X|C_k) \\cdot P(C_k)}{P(X)} $$\n",
    "\n",
    "- **$P(C_k|X)$** is the **posterior probability**: Probability of class $C_k$ given features $X$.\n",
    "- **$P(C_k)$** is the **prior probability**: Probability of class $C_k$.\n",
    "- **$P(X|C_k)$** is the **likelihood**: Likelihood of features $X$ given class $C_k$.\n",
    "- **$P(X)$** is the **marginal probability**: Evidence, the total probability of observing features $X$.\n",
    "\n",
    "### <font color='red'>Warning!</font> How do we deal with $X$ being multidimensional? <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c64e21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Complexity of High Dimensionality\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Direct calculation** of $P(X|C_k)$ involves understanding complex relationships among all features.\n",
    "\n",
    "<br>\n",
    "\n",
    "- With **multidimensional feature vectors** $X = (x_1, x_2, ..., x_n)$, calculating the likelihood, $P(X|C_k)$, directly becomes impractical due to the **curse of dimensionality**.\n",
    "\n",
    "<br>\n",
    "\n",
    "- High-dimensional spaces increase the **data requirement exponentially**. With 10 binary features, there are $2^{10} = 1024$ possible combinations, for example.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79af113",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>The Naive Assumption</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35bb43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Naive Assumption\n",
    "\n",
    "The Naive Bayes assumption simplifies the problem by assuming **each feature $x_i$ is independent of every other feature**.\n",
    "\n",
    "By treating each feature as independent, **we only need to calculate the probability of each feature individually given the class**, rather than all possible combinations of features.\n",
    "\n",
    "This leads to:\n",
    "\n",
    "$$ P(X|C_k) = P(x_1, x_2, ..., x_n|C_k) = \\prod_{i=1}^{n} P(x_i|C_k) $$\n",
    "\n",
    "Thus, we can rewrite the Bayes' Theorem and our classifier becomes:\n",
    "\n",
    "$$ P(C_k|X) = \\frac{P(C_k) \\prod_{i=1}^{n} P(x_i|C_k)}{P(X)} $$\n",
    "\n",
    "- **Independence**: Each feature $x_i$ is independent of every other feature given the class $C_k$.\n",
    "- This assumption significantly reduces computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd049bbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>The Naive Bayes Classifier</center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305e728",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Naive Bayes Classifier\n",
    "\n",
    "\n",
    "**Classification Decision**: Since $P(X)=P(x_1, \\dots, x_n) $  is constant across all classes, we focus on the classification rule that maximize the numerator:\n",
    "\n",
    "$$ \n",
    "P(C_k|X) = \\frac{P(C_k) \\prod_{i=1}^{n} P(x_i|C_k)}{P(X)} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P(C_k \\mid x_1, \\dots, x_n) \\propto P(C_k) \\prod_{i=1}^{n} P(x_i \\mid C_k)\n",
    "$$\n",
    "$$\n",
    "\\Downarrow\n",
    "$$\n",
    "\n",
    "$$ \\hat{C} = \\arg \\max_{C_k} P(C_k) \\prod_{i=1}^{n} P(x_i|C_k) $$\n",
    "\n",
    "$ P(C_k) $ is then the relative frequency of class $ C $\n",
    "in the training set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e48b28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Naive Bayes Classifier in Statistical Learning\n",
    "\n",
    "### Naive Bayes within Supervised Learning\n",
    "\n",
    "Naive Bayes classifiers belong to the family of supervised learning models for classification, differentiating itself from unsupervised techniques that focus on discovering hidden patterns in unlabeled data.\n",
    "\n",
    "### Naive Bayes as a Generative Model\n",
    "\n",
    "- **Naive Bayes** is a fundamental example of a generative model.\n",
    "\n",
    "- By learning the distribution of each class, Naive Bayes models the generation process of the data.\n",
    "\n",
    "- This allows not only for classification but also for generating new data samples based on the learned distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4f2db3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Naive Bayes Classifiers</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd16cf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing the Right Naive Bayes Classifier\n",
    "\n",
    "The best classifier aligns with the statistical properties of your data and performs best empirically.\n",
    "    \n",
    "## Selection Strategy <a class=\"tocSkip\">\n",
    "\n",
    "1. **Features Distribution**: How do we assume the features $x_i$ are distributed?\n",
    "2. **Domain Knowledge**: Let insights from the domain guide your choice.\n",
    "3. **Analyze Features**: Understand the distribution of your data (plot your data!).\n",
    "4. **Preprocess**: Tailor preprocessing to fit the model's assumptions (e.g. log transformations).\n",
    "5. **Model Comparison**: Apply different models and evaluate their performance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d496c2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Naive Bayes\n",
    "\n",
    "- **Assumption**: The features $x_i$ are assumed to be normally distributed (Gaussian) for each class $C_k$.\n",
    "- **Applicability**: Ideal for datasets where features are continuous.\n",
    "- **`Python`**: `GaussianNB` in the `scikit-learn` package implements the Gaussian Naive Bayes algorithm for classification.\n",
    "\n",
    "\n",
    "$$ P(x_i | C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{k}^{2}}} \\exp\\left(-\\frac{(x_i - \\mu_{k})^2}{2\\sigma_{k}^{2}}\\right) $$\n",
    "\n",
    "- Where $\\mu_{k}$ and $\\sigma_{k}^{2}$ are the mean and variance of feature $x_i$ for class $C_k$. They are estimated using [maximum likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#:~:text=In%20statistics%2C%20maximum%20likelihood%20estimation,observed%20data%20is%20most%20probable.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c9f22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multinomial Naive Bayes\n",
    "\n",
    "- **Assumption**: The features $x_i$ represent the frequencies with which certain events have been generated by a multinomial distribution.\n",
    "- **Applicability**: Suited for count data, such as the frequency of words in text documents.\n",
    "- **`Python`**: `MultinomialNB` in the `scikit-learn` package implements the algorithm. The distribution is parametrized by vectors $ \\theta_{C_k} = (\\theta_{C_k1},\\ldots,\\theta_{C_kn}) $\n",
    "for each class $ C_k $, where $ n $ is the number of features and $ \\theta_{C_ki} $ is the likelihood, $ P(x_i \\mid C_k) $, of feature $ i $ appearing in a sample belonging to class $ C_k $.\n",
    "\n",
    "The parameters $ \\theta_{C_k} $ is estimated by relative frequency counting:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{C_ki} = \\frac{ N_{C_ki} + \\alpha}{N_{C_k} + \\alpha n}\n",
    "$$\n",
    "\n",
    "where $ N_{C_ki} = \\sum_{x \\in T} x_i $ is the number of times feature $ i $ appears in a sample of class $ C_k $, and $ N_{C_k} = \\sum_{i=1}^{n} N_{C_ki} $ is the total count of\n",
    "all features for class $ C_k $.\n",
    "\n",
    "If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, because the probability estimate is directly proportional to the number of occurrences of a feature's value. \n",
    "\n",
    "\n",
    "\n",
    "The smoothing priors $ \\alpha > 0 $ accounts for features not present in the learning samples (*pseudocount*) and prevents zero probabilities in further computations. Setting $ \\alpha = 1 $ is called [Laplace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing), while $ \\alpha < 1 $ is called [Lidstone smoothing](https://en.wikipedia.org/wiki/Additive_smoothing).\n",
    "\n",
    "<br>\n",
    "\n",
    "Slide Navigation: <a href=\"#/118/1\">Link to Laplace Smoothing Example </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf254ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bernoulli Naive Bayes\n",
    "\n",
    "- **Assumption**: The features $x_i$ are binary (Boolean) variables indicating its presence or absence.\n",
    "\n",
    "- **Applicability**: Effective for datasets where features are binary, such as text classification where a word's presence or absence is a feature.\n",
    "\n",
    "- **`Python`**: `BernoulliNB` in the `scikit-learn` package implements the algorithm.\n",
    "\n",
    "The likelihood for Bernoulli Naive Bayes is\n",
    "\n",
    "$$\n",
    "P(x_i \\mid C_k) = P(x_i = 1 \\mid C_k) x_i + (1 - P(x_i = 1 \\mid C_k)) (1 - x_i)\n",
    "$$\n",
    "\n",
    "It explicitly penalizes the non-occurrence of a feature $ i $.\n",
    "\n",
    "<br>\n",
    "\n",
    "Slide Navigation: <a href=\"#/120/1\">Link to BNB Non-occurrence Example </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f4c2c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Measuring Performance\n",
    "\n",
    "<br>\n",
    "\n",
    "**Confusion Matrix**: it is a powerful tool as it provides insights beyond overall accuracy, allowing for a detailed analysis of the model's effectiveness.\n",
    "\n",
    "<br>\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "<br>\n",
    "\n",
    "Slide Navigation: <a href=\"#/119/1\">Link to ROC/AUC </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85edb6bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\dfrac{\\text{correct predictions}}{\\text{total observations}} \\ = \\ \\dfrac{tp + tn}{tp + tn + fp + fn}$$\n",
    "\n",
    "- Overall effectiveness of the model.\n",
    "- In the context of weather forecasting, accuracy would reflect how well a model predicts weather events correctly, such as correctly forecasting a day as rainy (true positive) or sunny (true negative), over the total number of forecasts made. \n",
    "- **High accuracy**: lots of correct predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a4b8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Precision\n",
    "\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "<br>\n",
    "    \n",
    "$$\\dfrac{\\text{true positives}}{\\text{total predicted positive}} = \\dfrac{tp}{tp + fp}$$\n",
    "\n",
    "- Accuracy of positive predictions.\n",
    "- In email spam detection, it would indicate the percentage of emails correctly identified as spam (true positives) out of all emails flagged as spam, aiming to reduce the number of legitimate emails incorrectly marked as spam (false positives).\n",
    "- **High precision**: low false-positive rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa64817",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recall (Sensitivity)\n",
    "\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\dfrac{\\text{true positives}}{\\text{total actual positive}} \\ = \\ \\dfrac{tp}{tp + fn}$$\n",
    "\n",
    "\n",
    "- Is the fraction of positives correctly identified.\n",
    "- In criminal justice, it would assess how well a predictive policing model identifies all potential criminal activities (true positives) without missing any (thus minimizing false negatives).\n",
    "- **High recall**: low false-negative rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8d88e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Specificity\n",
    "\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\text{true negatives}}{\\text{total actual negative}} = \\frac{tn}{tn + fp}$$\n",
    "\n",
    "- It is the true negative rate, measures a model's ability to correctly identify actual negatives\n",
    "- Crucial in fields where incorrectly identifying a negative case as positive could have serious implications (e.g., criminal justice).\n",
    "- **High specificity**: the model is very effective at identifying true negatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60387f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## F1-Score\n",
    "\n",
    "\n",
    "|  | **Predicted: 0** | **Predicted: 1** |\n",
    "|---|---|---|\n",
    "| **Actual: 0** | True Negative | False Positive |\n",
    "| **Actual: 1** | False Negative | True Positive |\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "$$ \\text{F1} \\ = \\ 2 \\times \\dfrac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}} $$\n",
    "\n",
    "- Harmonic mean of Precision and Recall.\n",
    "- In a medical diagnosis scenario, it would help in evaluating a test's effectiveness in correctly identifying patients with a disease (true positives) while minimizing the misclassification of healthy individuals as diseased (false positives and false negatives).\n",
    "- **High F1 score**: a better balance between precision and recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c961d2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Let's Practice!</center>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Slide Navigation: <a href=\"#/30/0\">Link to Gaussian Naive Bayes for Disease Prediction</a>\n",
    "\n",
    "- Slide Navigation: <a href=\"#/66/0\">Link to Multinomial Naive Bayes for Document Classfication</a>\n",
    "\n",
    "- Slide Navigation: <a href=\"#/88/1\">Link to Bernoulli Naive Bayes for Image Recognition</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceba1e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice: Gaussian Naive Bayes\n",
    "\n",
    "## Pima Indians Diabetes Database  <a class=\"tocSkip\">\n",
    "\n",
    "### Objective <a class=\"tocSkip\">   \n",
    "    \n",
    "- The aim is to predict whether a patient has diabetes based on diagnostic measurements. \n",
    "    \n",
    "### Context <a class=\"tocSkip\">\n",
    "\n",
    "- This dataset ([Kaggle Dataset Link](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database/data)) originates from the **National Institute of Diabetes and Digestive and Kidney Diseases**. \n",
    "- All patients are females of at least 21 years old of **Pima Indian heritage**, selected under specific constraints for a study.\n",
    "\n",
    "### Assumption <a class=\"tocSkip\">\n",
    "    \n",
    "- The features $x_i$ can be aproximated by a Gaussian distribution.    \n",
    "\n",
    "<br>\n",
    "    \n",
    "Slide Navigation: <a href=\"#/29/1\">Link to Practice Menu </a>\n",
    "    \n",
    "\n",
    "Slide Navigation: <a href=\"#/113/1\">Link to Audience Questions </a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8281e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Import libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67732bf8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing dataset\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/diabetes.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a361a295",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ee493",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Preview data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd32c17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset dimensions - (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff8d10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Features data-type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80e8a8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ee924",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Count of null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e0b2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Observations: <a class=\"tocSkip\">\n",
    "\n",
    "1. There are a total of 768 records and 9 features in the dataset.\n",
    "2. Each feature can be either of integer or float dataype.\n",
    "3. Some features like Glucose, Blood pressure, Insulin, BMI have zero values which represent missing data.\n",
    "4. There are zero NaN values in the dataset.\n",
    "5. In the outcome column, 1 represents diabetes positive and 0 represents diabetes negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26eba96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Outcome countplot\n",
    "sns.countplot(x = 'Outcome',data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d34d43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Histogram of each feature\n",
    "\n",
    "col = df.columns[:8]\n",
    "plt.subplots(figsize = (20, 15))\n",
    "length = len(col)\n",
    "\n",
    "for i, j in itertools.zip_longest(col, range(length)):\n",
    "    plt.subplot((length//2), 3, j + 1)\n",
    "    plt.subplots_adjust(wspace = 0.1,hspace = 0.5)\n",
    "    df[i].hist(bins = 20)\n",
    "    plt.title(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494a0b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Scatter plot matrix \n",
    "scatter_matrix(df, figsize = (20, 20));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694147e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Pairplot \n",
    "sns.pairplot(data = df, hue = 'Outcome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f05180",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "sns.heatmap(df.corr(), annot = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280b439",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Observations: <a class=\"tocSkip\">\n",
    "\n",
    "1. The countplot tells us that the dataset is imbalanced, as number of patients who don't have diabetes is more than those who do.\n",
    "\n",
    "2. From the correaltion heatmap, we can see that there is a high correlation between Outcome and [Glucose,BMI,Age,Insulin]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c40a7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da32c6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_new = df\n",
    "# list(df_new)\n",
    "# Preview data\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71363cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# checking zero values\n",
    "\n",
    "# np.where(df_new['Glucose'] == 0)[0].shape\n",
    "# np.where(df_new['BloodPressure'] == 0)[0].shape\n",
    "np.where(df_new['SkinThickness'] == 0)[0].shape\n",
    "# np.where(df_new['Insulin'] == 0)[0].shape\n",
    "# np.where(df_new['BMI'] == 0)[0].shape\n",
    "# np.where(df_new['DiabetesPedigreeFunction'] == 0)[0].shape\n",
    "# np.where(df_new['Age'] == 0)[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cdd3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Replacing zero values with NaN\n",
    "df_new[['Glucose',\n",
    " 'BloodPressure',\n",
    " 'SkinThickness',\n",
    " 'Insulin',\n",
    " 'BMI',\n",
    " 'DiabetesPedigreeFunction',\n",
    " 'Age']] = df_new[['Glucose',\n",
    " 'BloodPressure',\n",
    " 'SkinThickness',\n",
    " 'Insulin',\n",
    " 'BMI',\n",
    " 'DiabetesPedigreeFunction',\n",
    " 'Age']].replace(0, np.NaN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1081c2e4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Count of NaN\n",
    "df_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d3cb72",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Removing Features with too many zeros NaNs\n",
    "df_new = df_new.drop(['SkinThickness', 'Insulin'], axis = 1)\n",
    "\n",
    "# Removing Observations with NaNs\n",
    "df_new = df_new.dropna(subset=['Glucose'])\n",
    "df_new = df_new.dropna(subset=['BloodPressure'])\n",
    "df_new = df_new.dropna(subset=['BMI'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdddbd1a",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df_new.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee4e8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# log transformation\n",
    "df_new['LogPregnancies'] = np.log1p(df_new['Pregnancies'])\n",
    "df_new['LogDiabetesPedigreeFunction'] = np.log1p(df_new['DiabetesPedigreeFunction'])\n",
    "df_new['LogAge'] = np.log1p(df_new['Age'])\n",
    "\n",
    "# Statistical summary\n",
    "df_new.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a5324",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Histogram of each feature\n",
    "\n",
    "col = df_new.columns[:10]\n",
    "plt.subplots(figsize = (20, 15))\n",
    "length = len(col)\n",
    "\n",
    "for i, j in itertools.zip_longest(col, range(length)):\n",
    "    plt.subplot((length//2), 3, j + 1)\n",
    "    plt.subplots_adjust(wspace = 0.1,hspace = 0.5)\n",
    "    df_new[i].hist(bins = 20)\n",
    "    plt.title(i)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440008c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Selecting features \n",
    "features = [\n",
    "    'LogPregnancies',\n",
    "    'Glucose',\n",
    "    'BloodPressure',\n",
    "    'BMI',\n",
    "    'LogDiabetesPedigreeFunction',\n",
    "    'LogAge'\n",
    "]\n",
    "\n",
    "# Splitting X and Y\n",
    "df_train, df_test = train_test_split(df_new, test_size = 0.20, random_state = 42, stratify = df_new['Outcome'] )\n",
    "\n",
    "X_train = df_train[features]\n",
    "Y_train = df_train['Outcome']\n",
    "X_test = df_test[features]\n",
    "Y_test = df_test['Outcome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8c802",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Checking dimensions\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e20b005",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ecfda4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Algorithm\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e0afba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Making predictions on test dataset\n",
    "\n",
    "Y_pred_nb = nb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bd1f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d4a02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluating using accuracy_score metric\n",
    "\n",
    "accuracy_nb = accuracy_score(Y_test, Y_pred_nb)\n",
    "\n",
    "# Accuracy on test set\n",
    "print(\"Naive Bayes ACC: \" + str(accuracy_nb * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad454c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_test, Y_pred_nb)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60002e02",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Heatmap of Confusion matrix\n",
    "sns.heatmap(pd.DataFrame(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341f836",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Classification report\n",
    "\n",
    "print(classification_report(Y_test, Y_pred_nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db30139",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Cross Validation\n",
    "X = df_new[features]\n",
    "y = df_new['Outcome']\n",
    "result = cross_val_score(nb, X, y, scoring = 'accuracy')\n",
    "# Recall: If we consider that the cost of not classifying someone with diabetes is high, \n",
    "# that failing to identify a sick patient (a false negative) is more dangerous \n",
    "# than incorrectly diagnosing a healthy patient as sick (a false positive).\n",
    "\n",
    "result.mean(), result.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e90706",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice: Multinomial Naive Bayes\n",
    "\n",
    "### BBC Full Text Document Classification <a class=\"tocSkip\">\n",
    "\n",
    "### Objective <a class=\"tocSkip\">\n",
    "    \n",
    "- The aim is to predict which topic does a news article belong to based on its content.\n",
    "    \n",
    "### Context <a class=\"tocSkip\">\n",
    "\n",
    "- The original dataset ([Kaggle Dataset](https://www.kaggle.com/datasets/dheemanthbhat/bbc-full-text-preprocessed)) \n",
    "consists of 2225 documents (as text files) from the BBC news website corresponding to news articles in five topical areas: \n",
    "    - business\n",
    "    - entertainment \n",
    "    - politics\n",
    "    - sport\n",
    "    - tech\n",
    "\n",
    "### Assumption <a class=\"tocSkip\">\n",
    "    \n",
    "- The features $x_i$ represent the frequencies with which certain events have been generated by a multinomial distribution.    \n",
    "\n",
    "<br>\n",
    "    \n",
    "Slide Navigation: <a href=\"#/29/1\">Link to Practice Menu </a>\n",
    "\n",
    "Slide Navigation: <a href=\"#/113/1\">Link to Audience Questions </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759c9f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f216f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "train_df: pd.DataFrame = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/mnist_train_subset.csv')\n",
    "test_df: pd.DataFrame = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/mnist_test.csv')\n",
    "\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/bbc_text_cls.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97500f8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Preview data\n",
    "df.head(10)\n",
    "\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae01cb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680bbc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's check labels frequency\n",
    "\n",
    "df['labels'].hist(figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601556d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0f81d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Handles the removal of stopwords and stemming\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove punctuation and make lower case\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Stem the words\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284615f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the preprocessing to each row\n",
    "\n",
    "df['processed_text'] = df['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf96799",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de86d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into features and labels\n",
    "features = df['processed_text']\n",
    "labels = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed721a14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, \n",
    "                                                                            test_size=0.2, \n",
    "                                                                            random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad4251",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a text processing and classification pipeline\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    # Bag of Words\n",
    "    # Convert the processed text into a matrix of token counts, \n",
    "    # which is then used as input to the MultinomialNB classifier\n",
    "    CountVectorizer(),\n",
    "    MultinomialNB()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ace1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b3cb9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "pipeline.fit(features_train, labels_train)\n",
    "\n",
    "## Access the CountVectorizer step and get the feature names, which correspond to the number of features\n",
    "#num_features = len(pipeline.named_steps['countvectorizer'].get_feature_names_out())\n",
    "\n",
    "#print(f\"The number of features is: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb6ee7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "labels_pred = pipeline.predict(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77d9104",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3673325",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model using train-test split\n",
    "print(\"Train-test split evaluation:\")\n",
    "print(classification_report(labels_test, labels_pred))\n",
    "print(f\"Accuracy: {accuracy_score(labels_test, labels_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7017d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the model using cross-validation\n",
    "\n",
    "cross_val_accuracy = cross_val_score(pipeline, features_train, labels_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"\\nCross-validation evaluation:\")\n",
    "print(f\"Cross-validated accuracy: {np.mean(cross_val_accuracy)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d56b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion Matrix Display\n",
    "ConfusionMatrixDisplay.from_predictions(labels_test, labels_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47e0e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's Check Some Misclassified Examples\n",
    "\n",
    "# identifying misclassified examples\n",
    "misclassified_idx = np.where(labels_pred != labels_test)[0]\n",
    "\n",
    "# random select a misclassified example\n",
    "i = np.random.choice(misclassified_idx)\n",
    "\n",
    "print(\"True class:\", labels_test.iloc[i])\n",
    "print(\"Predicted class:\", labels_pred[i])\n",
    "\n",
    "# The specific element from 'features_test'\n",
    "specific_element = features_test.iloc[i]\n",
    "\n",
    "# Find the indices where the 'text' column in the DataFrame matches the specific element\n",
    "matching_indices = df.index[df['processed_text'] == specific_element].tolist()\n",
    "\n",
    "# Print text\n",
    "list(df.iloc[matching_indices,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95edd7a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the feature names and the log probability of features given a class\n",
    "feature_names = pipeline.named_steps['countvectorizer'].get_feature_names_out()\n",
    "feature_log_prob = pipeline.named_steps['multinomialnb'].feature_log_prob_\n",
    "\n",
    "# Create a DataFrame to hold the top words for each category\n",
    "top_words_per_category = pd.DataFrame()\n",
    "\n",
    "for i, category in enumerate(pipeline.named_steps['multinomialnb'].classes_):\n",
    "    # Get the indices of the top 10 features for this class\n",
    "    top_indices = np.argsort(feature_log_prob[i])[-10:]\n",
    "    # Get the associated words and probabilities\n",
    "    top_features = feature_names[top_indices]\n",
    "    top_probabilities = np.exp(feature_log_prob[i][top_indices])\n",
    "    # Add to the DataFrame\n",
    "    top_words_per_category[category] = top_features[::-1]  # Reverse to show the top word at first\n",
    "\n",
    "# Transpose the DataFrame to have categories as columns and words as rows\n",
    "top_words_per_category = top_words_per_category.T\n",
    "top_words_per_category.columns = [f'Top {i+1}' for i in range(top_words_per_category.shape[1])]\n",
    "\n",
    "# Print the table\n",
    "print(top_words_per_category.transpose())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa5515",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Practice: Bernoullil Naive Bayes\n",
    "\n",
    "### Objective  <a class=\"tocSkip\">\n",
    "\n",
    "- The aim is to predict the correct digit based on the hand-written image.\n",
    "\n",
    "### Context <a class=\"tocSkip\">\n",
    "\n",
    "- The original [Kaggle Dataset](https://www.kaggle.com/datasets/oddrationale/mnist-in-csv) contains the 60,000 training examples and labels in addition to 10,000 test examples and labels. Each row consists of 785 values: the first value is the label (a number from 0 to 9) and the remaining 784 values are the pixel values (a number from 0 to 255).\n",
    "    \n",
    "- For our purposes in the lecture, 33,000 random training examples were removed from the original dataset.\n",
    "        \n",
    "### Assumption <a class=\"tocSkip\">\n",
    "    \n",
    "- The features $x_i$ are binary (Boolean) variables indicating the presence or absence of a feature.\n",
    "    \n",
    "    \n",
    "<br>\n",
    "    \n",
    "Slide Navigation: <a href=\"#/29/1\">Link to Practice Menu </a>\n",
    "    \n",
    "\n",
    "Slide Navigation: <a href=\"#/113/1\">Link to Audience Questions </a>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30772e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1: Import libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c8b5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d139166",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df: pd.DataFrame = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/mnist_train_subset.csv')\n",
    "test_df: pd.DataFrame = pd.read_csv('https://raw.githubusercontent.com/davi-moreira/naive_bayes/main/data/mnist_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fc472",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662a169",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c894103",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1e83f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c32164",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dface53",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Sort the label counts by the label value, assuming they are categorical but not numerical\n",
    "label_counts = train_df['label'].value_counts().sort_index()\n",
    "\n",
    "# Create a bar plot for the label counts\n",
    "plt.figure(figsize=(12, 6))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.title('Distribution of Labels in the Dataset')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=1)  # Rotate x labels for better readability if necessary\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c361bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3c960",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create variables for the pixels and the labels we want to predict\n",
    "X_train: np.ndarray = train_df.drop('label', axis=1).to_numpy()\n",
    "y_train: np.ndarray = train_df['label'].to_numpy()\n",
    "X_test: np.ndarray = test_df.drop('label', axis=1).to_numpy()\n",
    "y_test: np.ndarray = test_df['label'].to_numpy()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ec1eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#print(X_train.ndim)\n",
    "#print(X_train.shape)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fbe73f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3813de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a sample of each digit as the original image\n",
    "# create sub plot for each digit\n",
    "fig, ax = plt.subplots(2,5, figsize=(7,4))\n",
    "# loop over each subplot to add its digit\n",
    "for i, ax in enumerate(ax.flatten()):\n",
    "    # find index for image with the corresponding digit\n",
    "    img_idx: int = np.argwhere(y_train == i)[0]\n",
    "    # get the image and reshape to 28X28\n",
    "    img: np.ndarray = np.reshape(X_train[img_idx], (28, 28))\n",
    "    # plot digit image\n",
    "    ax.imshow(img, cmap=\"gray_r\")\n",
    "    # add digit label\n",
    "    ax.set_title(f\"Label: {i}\")\n",
    "    # remove gridlines\n",
    "    ax.grid(False)\n",
    "# add title to the plot\n",
    "fig.suptitle(\"MNIST Images Sample And Their Labels\")\n",
    "# adjust the padding between and around subplots\n",
    "fig.tight_layout()\n",
    "# show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535e97e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We assume each pixel is either 0 (black) or 1 (white)\n",
    "# create empty list for our binary vector\n",
    "X_train_binary: List = []\n",
    "# loop over each vector\n",
    "for i in X_train:\n",
    "    # reshape digit vector to image\n",
    "    img: np.ndarray = np.reshape(i, (28, 28)).astype(np.uint8)\n",
    "    # binarize\n",
    "    im_gray: np.ndarray = cv2.threshold(\n",
    "        img, 120, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    # append to binary vector list\n",
    "    X_train_binary.append(np.reshape(im_gray, (784,)))\n",
    "# convert to numpy array\n",
    "X_train_binary: np.ndarray = np.asarray(X_train_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bed3b0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 4: Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17bc40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = BernoulliNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea21b0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2ebc9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "print(\"train acc:\", model.score(X_train, y_train))\n",
    "print(\"test acc:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6962b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Predict the labels for the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Print the classification report for the test set\n",
    "print(\"Classification report for the test set:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e57c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the confusion matrix for the test set\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Display the confusion matrix using ConfusionMatrixDisplay\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=model.classes_)\n",
    "cmd.plot(values_format='d')\n",
    "plt.title('Confusion Matrix for the Test Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c415137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation with 5 folds\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores for each fold\n",
    "print(\"Cross-validation scores for each fold:\", cv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00c715",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation of the cross-validation scores\n",
    "mean_cv_score = cv_scores.mean()\n",
    "std_cv_score = cv_scores.std()\n",
    "\n",
    "# Print the mean and standard deviation\n",
    "print(f\"Mean cross-validation accuracy: {mean_cv_score:.3f}\")\n",
    "print(f\"Standard deviation of cross-validation accuracy: {std_cv_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13076a67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Show some misclassified examples\n",
    "\n",
    "#np.random.seed(1)\n",
    "\n",
    "misclassified_idx = np.where(y_test_pred != y_test)[0]\n",
    "i = np.random.choice(misclassified_idx)\n",
    "plt.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
    "plt.title(f\"True label: {y_test[i]} Predicted: {y_test_pred[i]}\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f722f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Questions?</center> \n",
    "\n",
    "<br>\n",
    "    \n",
    "Slide Navigation: <a href=\"#/29/1\">Link to Practice Menu </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c7347",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework Assignment: Enhancing Naive Bayes Classifier Models\n",
    "\n",
    "## It is your turn! \n",
    "\n",
    "In this assignment, you are tasked with enhancing the predictive performance of one of the Naive Bayes classifier models we developed in class. This is an opportunity for you to implement and experiment with the machine learning concepts and techniques discussed during the lecture.\n",
    "\n",
    "### Objectives <a class=\"tocSkip\">\n",
    "\n",
    "- **Model Optimization**: Optimize the existing Naive Bayes classifier or select an alternative model that you believe could yield better results.\n",
    "- **Data Processing**: Apply different data preprocessing techniques to improve model accuracy.\n",
    "- **Research**: Conduct research to explore various strategies that could enhance model performance. Utilize reputable resources to support your choices.\n",
    "\n",
    "### Deliverables <a class=\"tocSkip\">\n",
    "\n",
    "1. **Enhanced Model Implementation**: A Python script or Jupyter Notebook containing the code for your improved model.\n",
    "2. **Performance Comparison**: A report comparing the original model's performance with your enhanced model. Include metrics such as accuracy, precision, recall, and F1-score.\n",
    "3. **Justification**: A detailed explanation of the changes you made, including:\n",
    "   - Why you chose to adjust or change the model.\n",
    "   - The data processing techniques you applied.\n",
    "   - Any resources or references you utilized to inform your decisions.\n",
    "\n",
    "### Evaluation Criteria <a class=\"tocSkip\">\n",
    "\n",
    "- **Innovation**: Creative and effective approaches to model enhancement.\n",
    "- **Accuracy**: The predictive performance of your final model.\n",
    "- **Justification**: The rationale behind your methodological choices.\n",
    "- **Presentation**: Clarity and structure of your comparative analysis and justifications.\n",
    "\n",
    "Your assignment will not only be evaluated on the improved accuracy of the model but also on your analytical approach and the ability to articulate your decision-making process.\n",
    "\n",
    "## <center>Have Fun!</center> <a class=\"tocSkip\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b71b94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Thank you!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b13b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "<br>\n",
    "\n",
    "- James, G., Witten, D., Hastie, T., Tibshirani, R., & Taylor, J. (2023). An Introduction to Statistical Learning: With Applications in Python (1st ed. 2023 edition). Springer.\n",
    "\n",
    "- H. Zhang (2004). [The optimality of Naive Bayes.](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf)\n",
    "  Proc. FLAIRS.  \n",
    "\n",
    "- C.D. Manning, P. Raghavan and H. SchÃ¼tze (2008). Introduction to\n",
    "  Information Retrieval. Cambridge University Press, pp. 234-265.  \n",
    "\n",
    "- A. McCallum and K. Nigam (1998).\n",
    "  [A comparison of event models for Naive Bayes text classification.](https://citeseerx.ist.psu.edu/doc_view/pid/04ce064505b1635583fa0d9cc07cac7e9ea993cc)\n",
    "  Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.  \n",
    "\n",
    "- V. Metsis, I. Androutsopoulos and G. Paliouras (2006).\n",
    "  [Spam filtering with Naive Bayes â€“ Which Naive Bayes?](https://citeseerx.ist.psu.edu/doc_view/pid/8bd0934b366b539ec95e683ae39f8abb29ccc757)\n",
    "  3rd Conf. on Email and Anti-Spam (CEAS).  \n",
    "\n",
    "- Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\n",
    "  [Tackling the poor assumptions of naive bayes text classifiers.](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf)\n",
    "  In ICML (Vol. 3, pp. 616-623).  \n",
    "\n",
    "- C.D. Manning, P. Raghavan and H. SchÃ¼tze (2008). Introduction to\n",
    "  Information Retrieval. Cambridge University Press, pp. 234-265.  \n",
    "\n",
    "- A. McCallum and K. Nigam (1998).\n",
    "  [A comparison of event models for Naive Bayes text classification.](https://citeseerx.ist.psu.edu/doc_view/pid/04ce064505b1635583fa0d9cc07cac7e9ea993cc)\n",
    "  Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.  \n",
    "\n",
    "- V. Metsis, I. Androutsopoulos and G. Paliouras (2006).\n",
    "  [Spam filtering with Naive Bayes â€“ Which Naive Bayes?](https://citeseerx.ist.psu.edu/doc_view/pid/8bd0934b366b539ec95e683ae39f8abb29ccc757)\n",
    "  3rd Conf. on Email and Anti-Spam (CEAS).  \n",
    "\n",
    "- Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\n",
    "  [Tackling the poor assumptions of naive bayes text classifiers.](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf)\n",
    "  In ICML (Vol. 3, pp. 616-623).  \n",
    "\n",
    "- Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). *Using the ADAP learning algorithm to forecast the onset of diabetes mellitus*. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.\n",
    "- D. Greene and P. Cunningham. \"Practical Solutions to the Problem of Diagonal Dominance in Kernel Document Clustering\", Proc. ICML 2006.\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5e128",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>Annex</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c5143",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Laplace smoothing Example <a id='laplace'></a>\n",
    "\n",
    "We have a dataset with 10 features (words) and 2 classes. We'll calculate the conditional probability $P(x_i|C_k)$  of a feature $x_i$ given a class $C_k$.\n",
    "\n",
    "Assume our dataset is represented as counts of each feature in documents belonging to each class. Here's a simplified representation:\n",
    "\n",
    "- Total documents in Class 1: 100\n",
    "- Total documents in Class 2: 100\n",
    "- Total number of features (words): 10\n",
    "\n",
    "Let's say we want to calculate $P(x_i|C_k)$ for a specific feature $x_3$ (the third word) for Class 1. Assume in our training data, $x_3$ appears 0 times in Class 1 documents.\n",
    "\n",
    "First, calculate the total count of all features in each class (let's assume these totals after counting all words in all documents):\n",
    "\n",
    "- Total feature counts in Class 1 documents: 500\n",
    "\n",
    "The probability of $x_3$ given Class 1, where $x_3$ does not appear at all, would be calculated simply as:\n",
    "\n",
    "$$\\hat{\\theta}_{C_13} = P(x_3|C_1) = \\frac{N_{x_3,C_1}}{N_{C_1}} = \\frac{0}{500} = 0$$\n",
    "\n",
    "Due to the Naive Assumption, this lead to a zero probability for any document containing $x_3$ to be classified as Class 1.\n",
    "\n",
    "### To solve this issue we use Laplace Smoothing <a class=\"tocSkip\">\n",
    "\n",
    "    \n",
    "### With Laplace Smoothing <a class=\"tocSkip\">\n",
    "\n",
    "For Laplace smoothing, where $\\alpha = 1$ and $n = 10$, even if $x_3$ does not appear in Class 1:\n",
    "\n",
    "$$\\hat{\\theta}_{C_13} = P(x_3|C_1) = \\frac{0 + 1}{500 + 10} = \\frac{1}{510} = 0.00196$$\n",
    "\n",
    "Due to the Naive Assumption, this non-zero probability allows the classifier to classify the documents with the unseen features in any class by assuming a small, non-zero probability for them.\n",
    "\n",
    "Slide Navigation: <a href=\"#/29/1\">Link to Practice Menu </a>\n",
    "    \n",
    "Slide Navigation: <a href=\"#/21/1\">Link to Multinomial Naive Bayes </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd934d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Model Performance with ROC Curve and AUC\n",
    "\n",
    "## ROC Curve <a class=\"tocSkip\">\n",
    "\n",
    "- Stands for Receiver Operating Characteristic Curve.\n",
    "- A graphical plot that illustrates the diagnostic ability of a binary classifier.\n",
    "- Plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n",
    "\n",
    "### True Positive Rate (TPR) <a class=\"tocSkip\">\n",
    "\n",
    "- Also known as sensitivity or recall.\n",
    "- Calculated as: $TPR = \\frac{TP}{TP + FN}$\n",
    "- Represents the proportion of actual positives that are correctly identified.\n",
    "\n",
    "### False Positive Rate (FPR) <a class=\"tocSkip\">\n",
    "\n",
    "- Calculated as: $FPR = \\frac{FP}{FP + TN}$\n",
    "- Represents the proportion of actual negatives that are incorrectly classified as positives.\n",
    "\n",
    "## AUC - Area Under the ROC Curve\n",
    "\n",
    "- A single scalar value summarizing the performance across all classification thresholds.\n",
    "- Ranges from 0 to 1, with **1 indicating a perfect model and 0.5 representing a random guess**.\n",
    "\n",
    "### Benefits of AUC <a class=\"tocSkip\">\n",
    "\n",
    "- AUC provides an aggregate measure of performance across all classification thresholds.\n",
    "- It's often used when the classes are very imbalanced.\n",
    "- AUC is scale-invariant and classification-threshold-invariant.\n",
    "\n",
    "## AUC Interpretation <a class=\"tocSkip\">\n",
    "    \n",
    "- A higher AUC value means a better-performing model. Model A with AUC of 0.85 is considered superior to Model B with an AUC of 0.75.\n",
    "- An AUC of 0.5 suggests no discriminative power, akin to random guessing.\n",
    "- AUC is particularly useful when you need to compare different models.\n",
    "\n",
    "## Usage <a class=\"tocSkip\">\n",
    "\n",
    "- Tools like Scikit-learn's `roc_curve` and `auc` functions can be used to compute ROC and AUC.\n",
    "\n",
    "```python\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Assuming y_true and y_pred have been defined:\n",
    "# y_true: actual class labels\n",
    "# y_pred: predicted probabilities or decision function values\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "Slide Navigation: <a href=\"#/29/1\">Link to Practice Menu </a>\n",
    "\n",
    "Slide Navigation: <a href=\"#/23/1\">Link to Measuring Performance </a>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e2f45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BNB Non-occurrence Example \n",
    "\n",
    "Let's consider a simple example involving email classification. We want to classify emails into two categories: \"Spam\" and \"Not Spam\" using a Bernoulli Naive Bayes classifier. We will use two binary features for this classifier:\n",
    "\n",
    "1. The word \"discount\" appears in the email ($x_1 $)\n",
    "2. The word \"lottery\" appears in the email ($x_2$)\n",
    "\n",
    "From our training data, we have calculated the following probabilities:\n",
    "\n",
    "- $P(\\text{\"discount\"} = 1 | \\text{Spam}) = 0.8 $\n",
    "- $P(\\text{\"lottery\"} = 1 | \\text{Spam}) = 0.9 $\n",
    "\n",
    "For the \"Not Spam\" class, the probabilities are:\n",
    "\n",
    "- $P(\\text{\"discount\"} = 1 | \\text{Not Spam}) = 0.1 $\n",
    "- $P(\\text{\"lottery\"} = 1 | \\text{Not Spam}) = 0.01 $\n",
    "\n",
    "Now, we receive a new email to classify that contains the word \"discount\" but not the word \"lottery\". The feature vector for this email is $ X = (x_1 = 1, x_2 = 0) $.\n",
    "\n",
    "For the \"Spam\" class:\n",
    "\n",
    "- The likelihood for $ x_1 $ (discount is present) contributes $ P(x_1 = 1 | \\text{Spam}) = 0.8 $.\n",
    "- The likelihood for $ x_2 $ (lottery is absent) contributes $ 1 - P(x_2 = 1 | \\text{Spam}) = 1 - 0.9 = 0.1 $.\n",
    "\n",
    "So, the total likelihood for the email being spam is $ 0.8 \\times 0.1 = 0.08 $.\n",
    "\n",
    "For the \"Not Spam\" class:\n",
    "\n",
    "- The likelihood for $ x_1 $ (discount is present) contributes $ P(x_1 = 1 | \\text{Not Spam}) = 0.1 $.\n",
    "- The likelihood for $ x_2 $ (lottery is absent) contributes $ 1 - P(x_2 = 1 | \\text{Not Spam}) = 1 - 0.01 = 0.99 $.\n",
    "\n",
    "So, the total likelihood for the email not being spam is $ 0.1 \\times 0.99 = 0.099 $.\n",
    "\n",
    "The absence of the highly indicative feature \"lottery\" significantly lowers the likelihood of the email being spam to 0.08. The likelihood of the email being \"Not Spam\" is actually higher than the likelihood of it being \"Spam\" (0.099 > 0.08).\n",
    "\n",
    "**Result**:\n",
    "\n",
    "The email would be classified as \"Not Spam\" because the likelihood of \"Not Spam\" (0.099) is higher than that of \"Spam\" (0.08), due to the significant penalty applied for the absence of the highly indicative feature \"lottery\".\n",
    "\n",
    "Slide Navigation: <a href=\"#/29/1\">Link to Practice Menu </a>\n",
    "\n",
    "Slide Navigation: <a href=\"#/22/1\">Link to Bernoulli Naive Bayes </a>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "date": 1706230122.46583,
  "filename": "naive_bayes.rst",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "rise": {
   "scroll": true
  },
  "title": "Naive Bayes",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "46.8125px",
    "left": "43.9688px",
    "top": "1348.08px",
    "width": "158.422px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
